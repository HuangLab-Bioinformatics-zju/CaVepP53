{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForSequenceClassification were not initialized from the model checkpoint at /home/huang/chy/ESMC/esm plus and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification,TrainerCallback\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "import scipy.special\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Define the working device\n",
    "import csv\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path='/home/huang/chy/ESMC/esm plus'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path,trust_remote_code=True).to(device).eval()\n",
    "tokenizer = model.tokenizer\n",
    "vocab = tokenizer.get_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "<class 'Bio.SeqRecord.SeqRecord'>\n",
      "<class 'Bio.Seq.Seq'>\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#############RHCH38序列\n",
    "\n",
    "TP53_pro=SeqIO.read(r\"/home/huang/chy/数据处理/p53_protein_sequence.fasta\", \"fasta\")\n",
    "print(len(TP53_pro))\n",
    "print(type(TP53_pro))\n",
    "TP53_pro_seq=TP53_pro.seq\n",
    "print(type(TP53_pro_seq))\n",
    "WT_seq=str(TP53_pro_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7467/7467 [07:44<00:00, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name REF  pos ALT  labels  \\\n",
      "0  M1K   M    1   K       2   \n",
      "1  M1R   M    1   R       2   \n",
      "2  M1H   M    1   H       2   \n",
      "3  M1E   M    1   E       2   \n",
      "4  M1D   M    1   D       2   \n",
      "\n",
      "                                              wt_seq  \\\n",
      "0  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...   \n",
      "1  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...   \n",
      "2  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...   \n",
      "3  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...   \n",
      "4  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...   \n",
      "\n",
      "                                             mut_seq  esm_distance_base  \\\n",
      "0  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...           0.118096   \n",
      "1  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...           0.081521   \n",
      "2  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...           0.073199   \n",
      "3  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...           0.125566   \n",
      "4  <pad><pad><pad><pad><pad><pad><pad><pad><pad><...           0.107556   \n",
      "\n",
      "   esm_distance_pretain  \n",
      "0              0.124961  \n",
      "1              0.085652  \n",
      "2              0.078810  \n",
      "3              0.130057  \n",
      "4              0.112156  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "esm_distance_list = []\n",
    "WT_avg_list=[]\n",
    "MUT_avg_list=[]\n",
    "# 遍历DataFrame\n",
    "for index, row in tqdm(df_gn.iterrows(), total=df_gn.shape[0]):\n",
    "    WT_seq = row['wt_seq']\n",
    "    MUT_seq = row['mut_seq']\n",
    "    \n",
    "    # 对WT序列进行编码\n",
    "    WT_seq_tokenized = tokenizer(WT_seq, padding=True, return_tensors='pt')\n",
    "    WT_seq_tokenized.to(device)\n",
    "    \n",
    "    # 获取WT序列的last_hidden_state\n",
    "    output = model(**WT_seq_tokenized)\n",
    "    WT_last_hidden_state = output.last_hidden_state\n",
    "    \n",
    "    # 对MUT序列进行编码\n",
    "    MUT_seq_tokenized = tokenizer(MUT_seq, padding=True, return_tensors='pt')\n",
    "    MUT_seq_tokenized.to(device)\n",
    "    \n",
    "    # 获取MUT序列的last_hidden_state\n",
    "    MUT_output = model(**MUT_seq_tokenized)\n",
    "    MUT_last_hidden_state = MUT_output.last_hidden_state\n",
    "    \n",
    "    # 计算整体嵌入向量\n",
    "    WT_avg_hidden_state = torch.mean(WT_last_hidden_state, dim=1)\n",
    "    MUT_avg_hidden_state = torch.mean(MUT_last_hidden_state, dim=1)\n",
    "    \n",
    "    # 计算欧氏距离\n",
    "    with torch.no_grad():\n",
    "        distance = torch.norm(WT_avg_hidden_state - MUT_avg_hidden_state)\n",
    "    WT_avg_hidden_state = WT_avg_hidden_state.squeeze().detach().cpu().numpy()\n",
    "    MUT_avg_hidden_state = MUT_avg_hidden_state.squeeze().detach().cpu().numpy()\n",
    "    # 将距离转换为CPU上的NumPy数组，并提取其值\n",
    "    distance = distance.cpu().numpy().item()\n",
    "    row['esm_distance_base'] = distance\n",
    "    #row['WT_rep'] = WT_last_hidden_state.squeeze().detach().cpu().numpy()\n",
    "    #row['MUT_rep'] = MUT_last_hidden_state.squeeze().detach().cpu().numpy()\n",
    "    esm_distance_list.append(distance)\n",
    "    WT_avg_list.append(WT_last_hidden_state.squeeze().detach().cpu().numpy())\n",
    "    MUT_avg_list.append(MUT_last_hidden_state.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    del WT_seq_tokenized,MUT_seq_tokenized,output,MUT_output\n",
    "    torch.cuda.empty_cache()\n",
    "#df_gn['WT_rep']=WT_avg_list\n",
    "#df_gn['MUT_rep']=MUT_avg_list\n",
    "df_gn['esm_distance_pretain']=esm_distance_list\n",
    "print(df_gn.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification,TrainerCallback\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "import scipy.special\n",
    "import os\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Define the working device\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "def run_inference(checkpoint_path, input_path, output_path, output_format):\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path,trust_remote_code=True).to(device)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer =  model.tokenizer\n",
    "\n",
    "    # Load data\n",
    "    data=pd.read_csv(input_path)\n",
    "\n",
    "\n",
    "    # Run model\n",
    "    results = []\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing\"):\n",
    "        seq=row['mut_seq']\n",
    "        inputs = tokenizer(seq, return_tensors=\"pt\", padding=True, truncation=True, max_length=601).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class = logits.argmax(dim=-1).item()\n",
    "        confidence = probabilities[0, predicted_class].item()\n",
    "        y_score = probabilities[0, 1].item()  # 获取正类（类别1）的概率\n",
    "        new_row=row.copy()\n",
    "        new_row['esmc_pre_label']=predicted_class\n",
    "        new_row['confidence_score']=confidence\n",
    "        new_row['y_score'] = y_score  # 添加 y_scores 列\n",
    "        results.append(new_row)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Save results\n",
    "    if output_format == \"json\":\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(results_df.to_dict(orient='records'), outfile, indent=4)\n",
    "    elif output_format == \"csv\":\n",
    "        with open(output_path, mode='w', newline='') as file:\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported output format. Use 'json' or 'csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as sp\n",
    "\n",
    "def run_inference(checkpoint_path, input_path, output_path, output_format):\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, trust_remote_code=True).to(device)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(input_path)\n",
    "\n",
    "    # Run model\n",
    "    results = []\n",
    "    esm_distance_list = []\n",
    "    WT_avg_list = []\n",
    "    MUT_avg_list = []\n",
    "\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing\"):\n",
    "        seq = row['mut_seq']\n",
    "        inputs = tokenizer(seq, return_tensors=\"pt\", padding=True, truncation=True, max_length=601).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class = logits.argmax(dim=-1).item()\n",
    "        confidence = probabilities[0, predicted_class].item()\n",
    "        y_score = probabilities[0, 1].item()  # 获取正类（类别1）的概率\n",
    "        new_row = row.copy()\n",
    "        new_row['esmc_pre_label'] = predicted_class\n",
    "        new_row['confidence_score'] = confidence\n",
    "        new_row['y_score'] = y_score  # 添加 y_scores 列\n",
    "        results.append(new_row)\n",
    "\n",
    "        WT_seq = row['wt_seq']\n",
    "        MUT_seq = row['mut_seq']\n",
    "\n",
    "        # 对序列进行编码\n",
    "        WT_seq_tokenized = tokenizer(WT_seq, padding=True, return_tensors='pt')\n",
    "        WT_seq_tokenized.to(device)\n",
    "        \n",
    "        # 获取WT序列的last_hidden_state\n",
    "        output = model(**WT_seq_tokenized)\n",
    "        WT_last_hidden_state = output.last_hidden_state\n",
    "        \n",
    "        # 对MUT序列进行编码\n",
    "        MUT_seq_tokenized = tokenizer(MUT_seq, padding=True, return_tensors='pt')\n",
    "        MUT_seq_tokenized.to(device)\n",
    "        \n",
    "        # 获取MUT序列的last_hidden_state\n",
    "        MUT_output = model(**MUT_seq_tokenized)\n",
    "        MUT_last_hidden_state = MUT_output.last_hidden_state\n",
    "        \n",
    "        # 计算整体嵌入向量\n",
    "        WT_avg_hidden_state = torch.mean(WT_last_hidden_state, dim=1)\n",
    "        MUT_avg_hidden_state = torch.mean(MUT_last_hidden_state, dim=1)\n",
    "        \n",
    "        # 计算欧氏距离\n",
    "        with torch.no_grad():\n",
    "            distance = torch.norm(WT_avg_hidden_state - MUT_avg_hidden_state)\n",
    "        WT_avg_hidden_state = WT_avg_hidden_state.squeeze().detach().cpu().numpy()\n",
    "        MUT_avg_hidden_state = MUT_avg_hidden_state.squeeze().detach().cpu().numpy()\n",
    "        # 将距离转换为CPU上的NumPy数组，并提取其值\n",
    "        distance = distance.cpu().numpy().item()\n",
    "        row['esm_distance_base'] = distance\n",
    "        row['WT_rep'] = WT_avg_hidden_state\n",
    "        #row['MUT_rep'] = MUT_last_hidden_state\n",
    "        esm_distance_list.append(distance)\n",
    "        WT_avg_list.append(WT_last_hidden_state.squeeze().detach().cpu().numpy())\n",
    "        MUT_avg_list.append(MUT_last_hidden_state.squeeze().detach().cpu().numpy())\n",
    "\n",
    "        del WT_seq_tokenized,MUT_seq_tokenized,output,MUT_output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Save results\n",
    "    if output_format == \"json\":\n",
    "        with open(output_path, 'w') as outfile:\n",
    "            json.dump(results_df.to_dict(orient='records'), outfile, indent=4)\n",
    "    elif output_format == \"csv\":\n",
    "        with open(output_path, mode='w', newline='') as file:\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported output format. Use 'json' or 'csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 705/705 [00:09<00:00, 72.37it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path='chenyuhe/CaVepP53'\n",
    "input_path='/home/huang/chy/P53预测最终/specific-model/最终评估结果总汇/test&val_data_finetune_compare.csv'\n",
    "output_path='/home/huang/chy/P53预测最终/specific-model/最终评估结果总汇/mode_test.csv'\n",
    "output_format=\"csv\"\n",
    "run_inference(checkpoint_path, input_path, output_path, output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate mut_sequence: 100%|██████████| 705/705 [00:00<00:00, 67856.54it/s]\n",
      "Predicting: 100%|██████████| 705/705 [00:31<00:00, 22.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from score import run_inference\n",
    "checkpoint_path='chenyuhe/CaVepP53'\n",
    "input_path='./data/example_pre_data.csv'\n",
    "output_path='./data/example_outcome.csv'\n",
    "run_inference(checkpoint_path, input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
